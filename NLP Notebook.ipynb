{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51a59349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0d23c",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing (NLP) \n",
    "\n",
    "> Natural language processing, which evolved from computational linguistics, uses methods from various disciplines, such as computer science, artificial intelligence, linguistics, and data science, to enable computers to understand human language in both written and verbal forms. \n",
    "[IBM](https://www.ibm.com/blogs/watson/2020/11/nlp-vs-nlu-vs-nlg-the-differences-between-three-natural-language-processing-concepts/)\n",
    "\n",
    "The usage of computers to have them the ability to understand both written and verbal forms in this case, text.\n",
    "Types of NLP:\n",
    "* Speech Recognition\n",
    "* Machine Translation\n",
    "* Sentiment Analysis\n",
    "* Semantic Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43903784",
   "metadata": {},
   "source": [
    "### Loading and Parsing through the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a600383",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./NLP-NLU-Files/Dataset/Disaster-Tweets/test.csv')\n",
    "train_df = pd.read_csv('./NLP-NLU-Files/Dataset/Disaster-Tweets/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c3409",
   "metadata": {},
   "source": [
    "#### Visualizing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f515fef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b122761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_shuffled = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c4ea3c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "@minsuwoongs i completely understand because i just woke up like 15 minutes ago and im Burning\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "im feeling attacked http://t.co/91jvYCxXVi\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real_disaster)\n",
      "Text:\n",
      "@mylittlepwnies3 @Early__May @AnathemaZhiv @TonySandos much of which has to do with lebanon 80s attack/ iran hostage crisis/ Libya Pan am\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "The Twitter update pretty much wrecked the app\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Season 3 of New Girl was such a emotional train wreck I just wanted to cry laugh and eat a lot of ice cream\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get a random sample set\n",
    "\n",
    "sample_set = train_df_shuffled[['text', 'target']]\n",
    "\n",
    "random_n = random.randint(0, len(sample_set)-5)\n",
    "\n",
    "for row in sample_set[random_n: random_n+5].itertuples():\n",
    "    _, text, target = row\n",
    "    print(f'Target: {target}', '(real_disaster)' if target > 0 else '(not real disaster)')\n",
    "    print(f'Text:\\n{text}\\n')\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbd96b",
   "metadata": {},
   "source": [
    "### Splitting the Train Data by Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9baa3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df_shuffled['text'].to_numpy()\n",
    "y = train_df_shuffled['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6e8c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bbf56f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 6851, 762, 762)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences),len(train_labels), len(val_sentences),len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81c1cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 real disaster \n",
      "LZK issues Severe Thunderstorm Warning [wind: 60 MPH hail: 0.75 IN] for Sharp [AR] till 8:15 PM CDT  http://t.co/AUXSMdG1uN #WX\n",
      "\n",
      "0 not real disaster \n",
      "@WoundedPigeon http://t.co/s9soAeVcVo Detonate by @ApolloBrown ft. M.O.P.\n",
      "\n",
      "0 not real disaster \n",
      "@CaraJDeIevingnc the bomb impact ratio hit beyond kyle js\n",
      "\n",
      "0 not real disaster \n",
      "Your brain is particularly vulnerable to trauma at two distinct ages http://t.co/KnBv2YtNWc @qz @TaraSwart @vivian_giang\n",
      "\n",
      "0 not real disaster \n",
      "i lava you! ????\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(f'{train_labels[i]}', 'real disaster' if train_labels[i] > 0 else 'not real disaster', \n",
    "          f'\\n{train_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82d4d3",
   "metadata": {},
   "source": [
    "### Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6154821a",
   "metadata": {},
   "source": [
    "---\n",
    "**Tokenization** is straight up mapping the words without any weight/values, just regular numerical encoding  \n",
    "  \n",
    "* **Word-Level Tokenization** - maps the whole text and maps each word; thus it(the word) is considered a token. (e.q. one-hot encoding)\n",
    "* **Character-Level Tokenization** - maps the whole text but focuses on each letter from 1 - 26; thus it(each letter) is considered a token.  \n",
    "* **Sub-word Tokenization** - takes the syllables of a word and tokenizes it \n",
    "\n",
    "**Embedding** uses vector weights that can be learned as out network trains. i.e. individual letters in a word on how each letter can be of importance to create that word.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ”‘ **Takeaways**  \n",
    "  \n",
    "**Process**  \n",
    "\n",
    "* **Build a Text Vectorizer**  \n",
    "    * **`max_vocab_length`** - 1000  \n",
    "    * **`max_length`** - taken from the length of each word in a sentence, summed together  \n",
    "and divided by the total amount of train_sentences  `sum([len(i.split()) for i in train_]) / len(train_)`\n",
    " \n",
    "\n",
    "* **Build a Embedding Layer**\n",
    "    * **`input_dim`** - same as `max_vocab_length`  \n",
    "    * **`output_dim`** - any number divisible by `8`  \n",
    "    * **`input_length`** - same as `max_length`  \n",
    "    \n",
    "     \n",
    "> You must vectorize your text before feeding it to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "96eec8be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.882206977083637"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(i.split()) for i in train_sentences])/len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2743c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_length = 10000\n",
    "max_length = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a7c08",
   "metadata": {},
   "source": [
    "#### Token Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "569c1cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_vocab_length,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length,\n",
    "    pad_to_max_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a00ebdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d38f7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test:\n",
      "@joshcorman  #infosec rather you knew it or not your a firefighter  now days  you often  run into burning buildings Deal with it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[   1, 5331, 1270,   12, 1825,   15,   58,   32,   34,    3, 1686,\n",
       "          51,  603,   12, 2860]], dtype=int64)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original test:\\n{random_sentence}')\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3346d684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['', '[UNK]', 'the', 'a', 'in'],\n",
       " ['pakistan\\x89Ã›Âªs', 'pakistans', 'pajamas', 'paints', 'painthey'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "words_in_vocab[:5],words_in_vocab[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb82c10",
   "metadata": {},
   "source": [
    "#### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cb2a72dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x233a57f2820>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=max_vocab_length,\n",
    "                                      output_dim=128,\n",
    "                                      input_length=max_length,\n",
    "                                      embeddings_initializer='uniform'\n",
    "                                     )\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e074668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test:\n",
      "@joshcorman  #infosec rather you knew it or not your a firefighter  now days  you often  run into burning buildings Deal with it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[-0.0110671 ,  0.01732009,  0.04375197, ..., -0.04271836,\n",
       "          0.00399405,  0.04798069],\n",
       "        [-0.00202902,  0.03009815,  0.0060215 , ...,  0.00019274,\n",
       "         -0.03650288,  0.03767592],\n",
       "        [-0.04458491,  0.04364324,  0.01128941, ...,  0.00490395,\n",
       "          0.02786065,  0.04195031],\n",
       "        ...,\n",
       "        [-0.00765001,  0.0353638 , -0.03011203, ..., -0.0300968 ,\n",
       "         -0.03033456, -0.04726434],\n",
       "        [-0.01424288, -0.03628627,  0.00607703, ..., -0.00190689,\n",
       "         -0.0046855 , -0.04964775],\n",
       "        [ 0.01280308, -0.04025245, -0.02600848, ..., -0.04596686,\n",
       "         -0.04381627, -0.01347705]]], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Original test:\\n{random_sentence}')\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a605af97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([-0.0110671 ,  0.01732009,  0.04375197,  0.00603743, -0.04594326,\n",
       "         0.01795593,  0.01034095, -0.04193724,  0.00294954, -0.03191074,\n",
       "        -0.01556091, -0.0371348 , -0.03070192,  0.00728117, -0.04708997,\n",
       "         0.02752427,  0.02307996, -0.02495719, -0.04091208,  0.00255727,\n",
       "         0.0403378 , -0.00195117,  0.04263123, -0.02346913, -0.00544274,\n",
       "        -0.04831278, -0.04629358, -0.00087645,  0.03791757, -0.01223205,\n",
       "        -0.04184105,  0.04388804, -0.04266124, -0.00613172,  0.0448084 ,\n",
       "         0.03379716,  0.04483197,  0.00587719,  0.03099189, -0.01697315,\n",
       "         0.035551  , -0.04736337,  0.02922549,  0.04674867,  0.02014044,\n",
       "        -0.02329115,  0.00762113, -0.04035987, -0.04018734,  0.02938296,\n",
       "         0.00176369,  0.01719406,  0.01451891, -0.03192464, -0.00386583,\n",
       "        -0.00457022, -0.02242173,  0.00988829, -0.03559873,  0.03894741,\n",
       "         0.01715103,  0.01431768, -0.01823659,  0.0296546 , -0.04988586,\n",
       "        -0.02865845, -0.02215099, -0.04378402, -0.04124746,  0.00883092,\n",
       "         0.0402002 , -0.00068206,  0.03550215,  0.03385376,  0.03838601,\n",
       "        -0.00265791,  0.0474746 ,  0.03885842, -0.00627385, -0.00836061,\n",
       "        -0.04125549, -0.04373617, -0.03104649, -0.0051767 ,  0.04691749,\n",
       "         0.01413386, -0.02135586, -0.01369747, -0.04613533,  0.02876432,\n",
       "         0.00956088, -0.0292358 ,  0.03377113,  0.03472369, -0.01714697,\n",
       "         0.01670733,  0.02519424,  0.00319324, -0.00752615,  0.01561618,\n",
       "         0.0243706 ,  0.03125271, -0.04291629, -0.02408136,  0.01509519,\n",
       "        -0.01530661, -0.00227301,  0.02604017, -0.04576936,  0.02906717,\n",
       "         0.01564946, -0.00175673, -0.02128456,  0.02384671, -0.00268065,\n",
       "         0.04273779,  0.01598312,  0.02278539, -0.04280386,  0.0130274 ,\n",
       "         0.02566806,  0.02622347,  0.01428887, -0.01724696, -0.04338708,\n",
       "        -0.04271836,  0.00399405,  0.04798069], dtype=float32)>,\n",
       " '@joshcorman  #infosec rather you knew it or not your a firefighter  now days  you often  run into burning buildings Deal with it.')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed[0][0], random_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca7e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
